{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQU7pJxQfod"
   },
   "source": [
    "# COMP47590 Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student Name:** \n",
    "- **Student Number:** \n",
    "\n",
    "\n",
    "- **Student Name:** \n",
    "- **Student Number:** \n",
    "\n",
    "\n",
    "- **Student Name:** \n",
    "- **Student Number:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPgBaQLxQfoj"
   },
   "source": [
    "## Assignment 2: Going the Distance\n",
    "Uses the PPO actor-critic method to train a neural network to control a simple robot in the RacingCar environment from OpenAI gym (https://gym.openai.com/envs/RacingCar-v0/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZNiGMqsQfok"
   },
   "source": [
    "![Racing](racing_car.gif)\n",
    "\n",
    "The **action** space can be continuous or discreet. If **continuous** there are 3 actions :\n",
    "\n",
    "- 0: steering, -1 is full left, +1 is full right\n",
    "- 1: gas\n",
    "- 2: breaking\n",
    "\n",
    "If **discrete** there are 5 actions:\n",
    "- 0: do nothing\n",
    "- 1: steer left\n",
    "- 2: steer right\n",
    "- 3: gas\n",
    "- 4: brake\n",
    "\n",
    "For this assignment we should use the continuous action space. \n",
    "\n",
    "**Reward** of -0.1 is awarded every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "And the default **observation** is a single image frame (96 * 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm_TwuoRQfol"
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHWFxiANQfom"
   },
   "source": [
    "If using Google colab you need to install packages - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XEhFeXkQfom",
    "outputId": "42e5ffd2-1cd3-4daf-b535-82e7c00c2dde"
   },
   "outputs": [],
   "source": [
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!pip install stable-baselines3[extra] pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9IM5FRQfoo"
   },
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still won't see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zh2cw7iSQfop"
   },
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Bcx9GkQfoq"
   },
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "dZcWCINuQfor"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd # For data frames and data frame manipulation\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np # For general  numeric operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEJsAysfQfos"
   },
   "source": [
    "### Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMqLB1VQfot"
   },
   "source": [
    "Create the **CarRacing-v2** environment. Add wrappers to resize the images and convert to greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7xkqRzcQfot",
    "outputId": "4a53b8ef-2971-4743-96b6-5172c8693737"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v3', \n",
    "               render_mode = 'human')\n",
    "env = gym.wrappers.ResizeObservation(env, (64,64))\n",
    "env = gym.wrappers.GrayscaleObservation(env, keep_dim = True)\n",
    "env = gym.wrappers.TimeLimit(env, \n",
    "                                max_episode_steps = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make('CarRacing-v3')\n",
    "env_train = gym.wrappers.ResizeObservation(env_train, (64,64))\n",
    "env_train = gym.wrappers.GrayscaleObservation(env_train, keep_dim = True)\n",
    "env_train = gym.wrappers.TimeLimit(env_train, \n",
    "                                max_episode_steps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9BQTzRKQfou"
   },
   "source": [
    "Explore the environment - view the action space and observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peo_ntRkQfou",
    "outputId": "e9170d3a-93e5-4305-89b8-6719ef0f4bde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgObNy4uQfov",
    "outputId": "f319f83d-c61b-47e8-ccac-99e5b39fe4f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (64, 64, 1), uint8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QsvRe0Qfow"
   },
   "source": [
    "Play an episode of the environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jvIhrjQfox",
    "outputId": "f12cbfa7-23a2-4efd-b58f-4729be5eb558"
   },
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "terminate = False\n",
    "truncate = False\n",
    "\n",
    "while not (terminate or truncate):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminate, truncate, info = env.step(action)\n",
    "    \n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ml3JnJ5Qfox"
   },
   "source": [
    "###Â Single Image Agent\n",
    "Create an agent that controls the car using a single image frame as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHY19WhjQfoy",
    "outputId": "38bcf412-18ea-40db-b6b5-b1bff5550f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "tb_log = './logs_carracing_PPO/'\n",
    "agent = sb3.PPO('CnnPolicy', \n",
    "                env_train, \n",
    "                verbose=1,\n",
    "                learning_rate=3e-5,\n",
    "                n_steps = 512,\n",
    "                ent_coef=0.001,\n",
    "                batch_size = 128,\n",
    "                gae_lambda = 0.9,\n",
    "                n_epochs = 20,\n",
    "                use_sde= True,\n",
    "                sde_sample_freq = 4,\n",
    "                clip_range = 0.4,\n",
    "                policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                tensorboard_log= tb_log\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVvGXkNQfoz"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjosaimQfoz",
    "outputId": "c64af8eb-3cb9-4d18-f1cd-c3251ea3904a"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRpUwTeQfoz"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oulh4r78Qfoz",
    "outputId": "876d8029-1ce4-46b7-e338-6643dadb8dd9"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMj3hpimQfo0"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNtGLDLUQfo0",
    "outputId": "1bc23c12-4702-4676-88b7-c5e7b632c887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs_carracing_PPO/PPO_3\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 19  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 26  |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 709         |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017301228 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | 3.5         |\n",
      "|    explained_variance   | 0.000727    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.692       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 1.6         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 709       |\n",
      "|    ep_rew_mean          | -110      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 13        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 112       |\n",
      "|    total_timesteps      | 1536      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.0918632 |\n",
      "|    clip_fraction        | 0.631     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | 1.29      |\n",
      "|    explained_variance   | -0.000161 |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 16.7      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 0.276     |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 96.4      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 854       |\n",
      "|    ep_rew_mean          | -84.7     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 13        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 151       |\n",
      "|    total_timesteps      | 2048      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7777246 |\n",
      "|    clip_fraction        | 0.749     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -3.29     |\n",
      "|    explained_variance   | -0.0767   |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 0.039     |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.0237    |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.0722    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 854         |\n",
      "|    ep_rew_mean          | -84.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065060124 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -3.7        |\n",
      "|    explained_variance   | 0.0906      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.256       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 903        |\n",
      "|    ep_rew_mean          | -77.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 234        |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12395972 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -4.5       |\n",
      "|    explained_variance   | 0.133      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.00654    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | 0.0209     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.0573     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 903         |\n",
      "|    ep_rew_mean          | -77.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 3584        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027570829 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.238       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.955       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 927        |\n",
      "|    ep_rew_mean          | -72        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 314        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08406468 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -5.32      |\n",
      "|    explained_variance   | 0.399      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.000282   |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.0322     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 927        |\n",
      "|    ep_rew_mean          | -72        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 356        |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07261397 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.596      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.216      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0525    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.617      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 942        |\n",
      "|    ep_rew_mean          | -64.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 411        |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09140576 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -5.43      |\n",
      "|    explained_variance   | 0.598      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.145      |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.399      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 942        |\n",
      "|    ep_rew_mean          | -64.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 445        |\n",
      "|    total_timesteps      | 5632       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06084959 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -5.54      |\n",
      "|    explained_variance   | 0.457      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.654      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 952       |\n",
      "|    ep_rew_mean          | -66.8     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 12        |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 491       |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1377661 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -6.02     |\n",
      "|    explained_variance   | 0.835     |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | -0.0349   |\n",
      "|    n_updates            | 220       |\n",
      "|    policy_gradient_loss | 0.00256   |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.0276    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 952         |\n",
      "|    ep_rew_mean          | -66.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 531         |\n",
      "|    total_timesteps      | 6656        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046064142 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -5.93       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.219       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.783       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 958         |\n",
      "|    ep_rew_mean          | -66.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037582874 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.00325     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 958       |\n",
      "|    ep_rew_mean          | -66.5     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 12        |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 598       |\n",
      "|    total_timesteps      | 7680      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0457486 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -6.62     |\n",
      "|    explained_variance   | 0.59      |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 0.109     |\n",
      "|    n_updates            | 280       |\n",
      "|    policy_gradient_loss | -0.0237   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.405     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 964        |\n",
      "|    ep_rew_mean          | -68.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 646        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05306807 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -6.72      |\n",
      "|    explained_variance   | 0.913      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | -0.0716    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0491    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.0216     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 964         |\n",
      "|    ep_rew_mean          | -68.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 8704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084727556 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -6.72       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.0746      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.605       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 968        |\n",
      "|    ep_rew_mean          | -67.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 718        |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04449327 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -6.49      |\n",
      "|    explained_variance   | 0.49       |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.144      |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.617      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 971         |\n",
      "|    ep_rew_mean          | -63         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 753         |\n",
      "|    total_timesteps      | 9728        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039474748 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 971         |\n",
      "|    ep_rew_mean          | -63         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 791         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031626426 |\n",
      "|    clip_fraction        | 0.0817      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.189       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.952       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x22a71dffe50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "terminate = False\n",
    "truncate = False\n",
    "\n",
    "while not (terminate or truncate):\n",
    "    \n",
    "    action, _states = agent.predict(obs, deterministic=True)\n",
    "    obs, reward, terminate, truncate, info = env.step(action)\n",
    "\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24RlrYF7Qfo0"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./logs_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nir4ZmnqQfo1"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "CBGtMv04Qfo1"
   },
   "outputs": [],
   "source": [
    "agent.save(\"./ppo_carracing_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxs3h7Owre5T"
   },
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uOGiJDrTrdV3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m agent\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m env\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43meval_env\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_env' is not defined"
     ]
    }
   ],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQF_FjMEQfo1"
   },
   "source": [
    "###Â Create Image Stack Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CarRacing-v0 environment using wrappers to resize the images to 64 x 64 and change to greyscale. Also add a wrapper to create a stack of 4 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijzQwZY0Qfo2"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77o0YzOmQfo2"
   },
   "source": [
    "Create an agent that controls the car using a stack of input image frames as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aAxmNANQfo2"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkOXkC-HQfo2"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS-fpmWwQfo3"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jObjvdQfo3"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL2c4LiOQfo4"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s33dR-mgQfo4"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRj1j3xjQfo4"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sotpjktQfo4"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./log_tb_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8j9GrTTQfo5"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8ivh_VpQfo5"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZrzrZ3Qfo5"
   },
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRzc_-yOQfo5"
   },
   "source": [
    "Load the single image saved agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyfQmsF6Qfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the single image environment for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8X8r5S5Qfo6"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrMN5vOeQfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6K6TXdCQfo6"
   },
   "source": [
    "For memory management delete the single image agent (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAeeHAKVQfo6"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1y0UngLQfo6"
   },
   "source": [
    "Load the image stack agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCxi6SvMQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the image stack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr8lGKjpQfo7"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgbbH_UkQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRCxNiPqQfo7"
   },
   "source": [
    "Reflect on which  agent performs better at the task, and the training process involved (max 200 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "WS3 Going The Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.6875px",
    "left": "1058px",
    "top": "147.125px",
    "width": "159.359px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
