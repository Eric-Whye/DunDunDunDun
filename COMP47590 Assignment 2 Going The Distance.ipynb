{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQU7pJxQfod"
   },
   "source": [
    "# COMP47590 Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student Name:** \n",
    "- **Student Number:** \n",
    "\n",
    "\n",
    "- **Student Name:** \n",
    "- **Student Number:** \n",
    "\n",
    "\n",
    "- **Student Name:** \n",
    "- **Student Number:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPgBaQLxQfoj"
   },
   "source": [
    "## Assignment 2: Going the Distance\n",
    "Uses the PPO actor-critic method to train a neural network to control a simple robot in the RacingCar environment from OpenAI gym (https://gym.openai.com/envs/RacingCar-v0/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZNiGMqsQfok"
   },
   "source": [
    "![Racing](racing_car.gif)\n",
    "\n",
    "The **action** space can be continuous or discreet. If **continuous** there are 3 actions :\n",
    "\n",
    "- 0: steering, -1 is full left, +1 is full right\n",
    "- 1: gas\n",
    "- 2: breaking\n",
    "\n",
    "If **discrete** there are 5 actions:\n",
    "- 0: do nothing\n",
    "- 1: steer left\n",
    "- 2: steer right\n",
    "- 3: gas\n",
    "- 4: brake\n",
    "\n",
    "For this assignment we should use the continuous action space. \n",
    "\n",
    "**Reward** of -0.1 is awarded every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "And the default **observation** is a single image frame (96 * 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm_TwuoRQfol"
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHWFxiANQfom"
   },
   "source": [
    "If using Google colab you need to install packages - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XEhFeXkQfom",
    "outputId": "42e5ffd2-1cd3-4daf-b535-82e7c00c2dde"
   },
   "outputs": [],
   "source": [
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!pip install stable-baselines3[extra] pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9IM5FRQfoo"
   },
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still won't see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh2cw7iSQfop"
   },
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Bcx9GkQfoq"
   },
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dZcWCINuQfor"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 12:31:21.128262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd # For data frames and data frame manipulation\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np # For general  numeric operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEJsAysfQfos"
   },
   "source": [
    "### Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMqLB1VQfot"
   },
   "source": [
    "Create the **CarRacing-v2** environment. Add wrappers to resize the images and convert to greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7xkqRzcQfot",
    "outputId": "4a53b8ef-2971-4743-96b6-5172c8693737"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', \n",
    "               render_mode = 'human')\n",
    "env = gym.wrappers.resize_observation.ResizeObservation(env, 64)\n",
    "env = gym.wrappers.gray_scale_observation.GrayScaleObservation(env, keep_dim = True)\n",
    "env = gym.wrappers.TimeLimit(env, \n",
    "                                max_episode_steps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9BQTzRKQfou"
   },
   "source": [
    "Explore the environment - view the action space and observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peo_ntRkQfou",
    "outputId": "e9170d3a-93e5-4305-89b8-6719ef0f4bde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgObNy4uQfov",
    "outputId": "f319f83d-c61b-47e8-ccac-99e5b39fe4f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (64, 64, 1), uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QsvRe0Qfow"
   },
   "source": [
    "Play an episode of the environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jvIhrjQfox",
    "outputId": "f12cbfa7-23a2-4efd-b58f-4729be5eb558"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ml3JnJ5Qfox"
   },
   "source": [
    "###Â Single Image Agent\n",
    "Create an agent that controls the car using a single image frame as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHY19WhjQfoy",
    "outputId": "38bcf412-18ea-40db-b6b5-b1bff5550f22"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVvGXkNQfoz"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjosaimQfoz",
    "outputId": "c64af8eb-3cb9-4d18-f1cd-c3251ea3904a"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRpUwTeQfoz"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oulh4r78Qfoz",
    "outputId": "876d8029-1ce4-46b7-e338-6643dadb8dd9"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMj3hpimQfo0"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNtGLDLUQfo0",
    "outputId": "1bc23c12-4702-4676-88b7-c5e7b632c887"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24RlrYF7Qfo0"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./logs_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nir4ZmnqQfo1"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBGtMv04Qfo1"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxs3h7Owre5T"
   },
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOGiJDrTrdV3"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQF_FjMEQfo1"
   },
   "source": [
    "###Â Create Image Stack Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CarRacing-v0 environment using wrappers to resize the images to 64 x 64 and change to greyscale. Also add a wrapper to create a stack of 4 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijzQwZY0Qfo2"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77o0YzOmQfo2"
   },
   "source": [
    "Create an agent that controls the car using a stack of input image frames as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aAxmNANQfo2"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkOXkC-HQfo2"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS-fpmWwQfo3"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jObjvdQfo3"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL2c4LiOQfo4"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s33dR-mgQfo4"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRj1j3xjQfo4"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sotpjktQfo4"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./log_tb_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8j9GrTTQfo5"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8ivh_VpQfo5"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZrzrZ3Qfo5"
   },
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRzc_-yOQfo5"
   },
   "source": [
    "Load the single image saved agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyfQmsF6Qfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the single image environment for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8X8r5S5Qfo6"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrMN5vOeQfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6K6TXdCQfo6"
   },
   "source": [
    "For memory management delete the single image agent (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAeeHAKVQfo6"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1y0UngLQfo6"
   },
   "source": [
    "Load the image stack agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCxi6SvMQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the image stack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr8lGKjpQfo7"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgbbH_UkQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRCxNiPqQfo7"
   },
   "source": [
    "Reflect on which  agent performs better at the task, and the training process involved (max 200 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "WS3 Going The Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.6875px",
    "left": "1058px",
    "top": "147.125px",
    "width": "159.359px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
