{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQU7pJxQfod"
   },
   "source": [
    "# COMP47590 Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student Name:** \n",
    "- **Student Number:** \n",
    "\n",
    "\n",
    "- **Student Name:** \n",
    "- **Student Number:** \n",
    "\n",
    "\n",
    "- **Student Name:** \n",
    "- **Student Number:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPgBaQLxQfoj"
   },
   "source": [
    "## Assignment 2: Going the Distance\n",
    "Uses the PPO actor-critic method to train a neural network to control a simple robot in the RacingCar environment from OpenAI gym (https://gym.openai.com/envs/RacingCar-v0/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZNiGMqsQfok"
   },
   "source": [
    "![Racing](racing_car.gif)\n",
    "\n",
    "The **action** space can be continuous or discreet. If **continuous** there are 3 actions :\n",
    "\n",
    "- 0: steering, -1 is full left, +1 is full right\n",
    "- 1: gas\n",
    "- 2: breaking\n",
    "\n",
    "If **discrete** there are 5 actions:\n",
    "- 0: do nothing\n",
    "- 1: steer left\n",
    "- 2: steer right\n",
    "- 3: gas\n",
    "- 4: brake\n",
    "\n",
    "For this assignment we should use the continuous action space. \n",
    "\n",
    "**Reward** of -0.1 is awarded every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "And the default **observation** is a single image frame (96 * 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm_TwuoRQfol"
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHWFxiANQfom"
   },
   "source": [
    "If using Google colab you need to install packages - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XEhFeXkQfom",
    "outputId": "42e5ffd2-1cd3-4daf-b535-82e7c00c2dde"
   },
   "outputs": [],
   "source": [
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!pip install stable-baselines3[extra] pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9IM5FRQfoo"
   },
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still won't see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zh2cw7iSQfop"
   },
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Bcx9GkQfoq"
   },
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dZcWCINuQfor"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd # For data frames and data frame manipulation\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np # For general  numeric operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEJsAysfQfos"
   },
   "source": [
    "### Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMqLB1VQfot"
   },
   "source": [
    "Create the **CarRacing-v2** environment. Add wrappers to resize the images and convert to greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7xkqRzcQfot",
    "outputId": "4a53b8ef-2971-4743-96b6-5172c8693737"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v3', \n",
    "               render_mode = 'human')\n",
    "env = gym.wrappers.ResizeObservation(env, (64,64))\n",
    "env = gym.wrappers.GrayscaleObservation(env, keep_dim = True)\n",
    "env = gym.wrappers.TimeLimit(env, \n",
    "                                max_episode_steps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9BQTzRKQfou"
   },
   "source": [
    "Explore the environment - view the action space and observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peo_ntRkQfou",
    "outputId": "e9170d3a-93e5-4305-89b8-6719ef0f4bde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgObNy4uQfov",
    "outputId": "f319f83d-c61b-47e8-ccac-99e5b39fe4f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (64, 64, 1), uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QsvRe0Qfow"
   },
   "source": [
    "Play an episode of the environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jvIhrjQfox",
    "outputId": "f12cbfa7-23a2-4efd-b58f-4729be5eb558"
   },
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "terminate = False\n",
    "truncate = False\n",
    "\n",
    "while not (terminate or truncate):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminate, truncate, info = env.step(action)\n",
    "    \n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ml3JnJ5Qfox"
   },
   "source": [
    "###Â Single Image Agent\n",
    "Create an agent that controls the car using a single image frame as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHY19WhjQfoy",
    "outputId": "38bcf412-18ea-40db-b6b5-b1bff5550f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "tb_log = './log_tb_highway_PPO/'\n",
    "agent = sb3.PPO('CnnPolicy', \n",
    "                env, \n",
    "                verbose=1,\n",
    "                learning_rate=3e-5,\n",
    "                n_steps = 512,\n",
    "                ent_coef=0.001,\n",
    "                batch_size = 128,\n",
    "                gae_lambda = 0.9,\n",
    "                n_epochs = 20,\n",
    "                use_sde= True,\n",
    "                sde_sample_freq = 4,\n",
    "                clip_range = 0.4,\n",
    "                policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                tensorboard_log= tb_log\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVvGXkNQfoz"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjosaimQfoz",
    "outputId": "c64af8eb-3cb9-4d18-f1cd-c3251ea3904a"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRpUwTeQfoz"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oulh4r78Qfoz",
    "outputId": "876d8029-1ce4-46b7-e338-6643dadb8dd9"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMj3hpimQfo0"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNtGLDLUQfo0",
    "outputId": "1bc23c12-4702-4676-88b7-c5e7b632c887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 16  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 30  |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -4.76     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 16        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 60        |\n",
      "|    total_timesteps      | 1024      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3699383 |\n",
      "|    clip_fraction        | 0.308     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | 3.83      |\n",
      "|    explained_variance   | -0.000654 |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 0.81      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0596   |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 1.79      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -4.76       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012925553 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | 2.86        |\n",
      "|    explained_variance   | -0.00373    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 0.639       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 16.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 15        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 131       |\n",
      "|    total_timesteps      | 2048      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9882936 |\n",
      "|    clip_fraction        | 0.422     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | 1.82      |\n",
      "|    explained_variance   | 0.0155    |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 1.75      |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.0581    |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 4.22      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 16.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 177        |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08468895 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.00646    |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.68       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 1.94       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -10.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 227        |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38733557 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -2.5       |\n",
      "|    explained_variance   | 0.358      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.174      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.453      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -10.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 261        |\n",
      "|    total_timesteps      | 3584       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17691976 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -3.61      |\n",
      "|    explained_variance   | 0.351      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.409      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.0272     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.582      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -24.1     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 13        |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 305       |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2593177 |\n",
      "|    clip_fraction        | 0.394     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -4.32     |\n",
      "|    explained_variance   | 0.652     |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 0.188     |\n",
      "|    n_updates            | 140       |\n",
      "|    policy_gradient_loss | 0.0348    |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.231     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -24.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 333        |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05296161 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -4.63      |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 0.0655     |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.468      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -34.9     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 361       |\n",
      "|    total_timesteps      | 5120      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1137236 |\n",
      "|    clip_fraction        | 0.213     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -5.37     |\n",
      "|    explained_variance   | 0.766     |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 0.0197    |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | 0.000443  |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.207     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x29c20de0ed0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24RlrYF7Qfo0"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./logs_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nir4ZmnqQfo1"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBGtMv04Qfo1"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxs3h7Owre5T"
   },
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOGiJDrTrdV3"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQF_FjMEQfo1"
   },
   "source": [
    "###Â Create Image Stack Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CarRacing-v0 environment using wrappers to resize the images to 64 x 64 and change to greyscale. Also add a wrapper to create a stack of 4 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijzQwZY0Qfo2"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77o0YzOmQfo2"
   },
   "source": [
    "Create an agent that controls the car using a stack of input image frames as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aAxmNANQfo2"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkOXkC-HQfo2"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS-fpmWwQfo3"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jObjvdQfo3"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL2c4LiOQfo4"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s33dR-mgQfo4"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRj1j3xjQfo4"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sotpjktQfo4"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./log_tb_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8j9GrTTQfo5"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8ivh_VpQfo5"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZrzrZ3Qfo5"
   },
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRzc_-yOQfo5"
   },
   "source": [
    "Load the single image saved agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyfQmsF6Qfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the single image environment for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8X8r5S5Qfo6"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrMN5vOeQfo6"
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6K6TXdCQfo6"
   },
   "source": [
    "For memory management delete the single image agent (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAeeHAKVQfo6"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1y0UngLQfo6"
   },
   "source": [
    "Load the image stack agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCxi6SvMQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the image stack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr8lGKjpQfo7"
   },
   "source": [
    "Evaluate the agent in the environment for 30 episodes, rendering the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgbbH_UkQfo7"
   },
   "outputs": [],
   "source": [
    "# Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRCxNiPqQfo7"
   },
   "source": [
    "Reflect on which  agent performs better at the task, and the training process involved (max 200 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "WS3 Going The Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.6875px",
    "left": "1058px",
    "top": "147.125px",
    "width": "159.359px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
